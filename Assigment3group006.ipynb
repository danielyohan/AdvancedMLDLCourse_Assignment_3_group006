{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOWMxMcmXssZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "tqdm.pandas()\n",
        "\n",
        "import spacy\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Conv1D, Flatten, Dense, Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorboard.plugins import projector\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/odedovadia/AdvancedMLDLCourse/main/Assignments/Assignment%20II%20-%20NLP/train.csv')\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/odedovadia/AdvancedMLDLCourse/main/Assignments/Assignment%20II%20-%20NLP/test.csv')"
      ],
      "metadata": {
        "id": "7sLY0sLeX75N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "xgyZp3-xYBNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "LxNMY8FLtpUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Show'].value_counts()"
      ],
      "metadata": {
        "id": "OrLu2pWWcDyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "4Sj5UTw0uBfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text cleaning"
      ],
      "metadata": {
        "id": "ia7_RYQdhhqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for removing special characters\n",
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "train_df['Dialogue'] = train_df['Dialogue'].apply(remove_special_characters)\n",
        "test_df['Dialogue'] = test_df['Dialogue'].apply(remove_special_characters)"
      ],
      "metadata": {
        "id": "GDZO2vH6lnEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = train_df['Dialogue'][1]\n",
        "sentence_1"
      ],
      "metadata": {
        "id": "dWHOeO5_ulwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stop words"
      ],
      "metadata": {
        "id": "IEjsc7M8os3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "# Create stop word list\n",
        "nltk.download('stopwords')\n",
        "stopword_list = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "faTDPUEEps_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the stopwords\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "# Apply function on review column\n",
        "train_df['Dialogue'] = train_df['Dialogue'].apply(remove_stopwords)\n",
        "test_df['Dialogue'] = test_df['Dialogue'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "AXyIZ_-7pxpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_2 = train_df['Dialogue'][1]\n",
        "print('before:', sentence_1,'\\n','after:', train_df['Dialogue'][1])"
      ],
      "metadata": {
        "id": "rahArSbp1dMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizer\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBW6fhHqvm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function for lemmatization\n",
        "def lemmatize_sentence(sentence):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    # Lemmatize each word in the sentence\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the lemmatized words back into a sentence\n",
        "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "\n",
        "    return lemmatized_sentence\n",
        "\n",
        "# Apply lemmatization to each sentence in the DataFrame\n",
        "train_df['Lemmatized_Dialogue'] = train_df['Dialogue'].apply(lemmatize_sentence)\n",
        "test_df['Lemmatized_Dialogue'] = test_df['Dialogue'].apply(lemmatize_sentence)\n",
        "# Display the DataFrame with lemmatized dialogue\n",
        "print(train_df[['Dialogue', 'Lemmatized_Dialogue']].head())\n"
      ],
      "metadata": {
        "id": "3XNwQ7srDoxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['Lemmatized_Dialogue'][3])\n",
        "print(train_df['Dialogue'][3])"
      ],
      "metadata": {
        "id": "6zkMphMoE_u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test-valdiation split"
      ],
      "metadata": {
        "id": "qDERuEzd8aqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df['Lemmatized_Dialogue']\n",
        "Y = train_df['Show']\n",
        "\n",
        "x_test = test_df['Lemmatized_Dialogue']\n",
        "y_test = test_df['Show']"
      ],
      "metadata": {
        "id": "C1oFgowALPx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "9Lsr67T78weB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the distribution of our labels between test and train is not skewed toward one class:"
      ],
      "metadata": {
        "id": "Yv0jFDEE80Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_count = y_train.value_counts()\n",
        "test_label_count = y_test.value_counts()\n",
        "\n",
        "print('Train negative to positive ratio:', train_label_count['Friends'] / train_label_count['Seinfeld'])\n",
        "print('Test negative to positive ratio:', test_label_count['Friends'] / test_label_count['Seinfeld'])"
      ],
      "metadata": {
        "id": "GZX_cIoU80Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print('x_val shape:', x_val.shape)\n",
        "print('y_val shape:', y_val.shape)"
      ],
      "metadata": {
        "id": "e-1qDEVn9EVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert 'Friends' to 0 and 'Seinfeld' to 1"
      ],
      "metadata": {
        "id": "sfRCkNESXOmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.transform(y_test)\n",
        "y_val = lb.transform(y_val)"
      ],
      "metadata": {
        "id": "cbCsFsY_XRZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "ACa3CGPlB1zN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhHiKKoVMJI0"
      },
      "outputs": [],
      "source": [
        "token = Tokenizer(lower=False)\n",
        "token.fit_on_texts(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzpmCCEmQ0J9"
      },
      "source": [
        "Convert to sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsGkOFx3Mp5q"
      },
      "outputs": [],
      "source": [
        "x_train_dl = token.texts_to_sequences(x_train)\n",
        "x_val_dl = token.texts_to_sequences(x_val)\n",
        "x_test_dl = token.texts_to_sequences(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0n41uwiQ0J9"
      },
      "outputs": [],
      "source": [
        "x_train_dl[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rdMt7bAQ0J9"
      },
      "source": [
        "Calculate document lengths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bijeEJzyQ0J9"
      },
      "outputs": [],
      "source": [
        "doc_len_list = [len(doc) for doc in x_train_dl]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnsWP9_wQ0J-"
      },
      "source": [
        "Longest, shortest, and average document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKg9Q6pqQ0J-"
      },
      "outputs": [],
      "source": [
        "max(doc_len_list), min(doc_len_list), np.mean(doc_len_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0cquxZnQ0J-"
      },
      "source": [
        "Let's examine the distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOKUlntZQ0J-"
      },
      "outputs": [],
      "source": [
        "sns.histplot(doc_len_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHZvS2uyQ0J-"
      },
      "source": [
        "50 seems like a reasonable shared length of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smk5cTOyQ0J-"
      },
      "outputs": [],
      "source": [
        "max_words = 50\n",
        "\n",
        "x_train_dl = sequence.pad_sequences(x_train_dl, maxlen=max_words)\n",
        "x_val_dl = sequence.pad_sequences(x_val_dl, maxlen=max_words)\n",
        "x_test_dl = sequence.pad_sequences(x_test_dl, maxlen=max_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRgKowy8Q0J-"
      },
      "source": [
        "Let's see what a sample look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMk45bdtUbc-"
      },
      "outputs": [],
      "source": [
        "total_words = len(token.word_index) + 1\n",
        "total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7J-6NBkQ0J-"
      },
      "outputs": [],
      "source": [
        "x_train_dl.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozLXZoO7wszJ"
      },
      "outputs": [],
      "source": [
        "x_train_dl[10, :]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "model.add(Embedding(input_dim=total_words, output_dim=32, input_length=max_words))\n",
        "\n",
        "# Bidirectional LSTM Layer (Return Sequences)\n",
        "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
        "\n",
        "# Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(50)))\n",
        "\n",
        "model.add(Dropout(0.6))  # Adjust the dropout rate as needed\n",
        "\n",
        "# Dense Layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(\n",
        "     loss='binary_crossentropy',\n",
        "     optimizer='adam',\n",
        "     metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YiwKRcTJ8Zz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train the model with a maximum of 20 epochs, and we'll set a condition to stop epoching when the accuracy no longer improves."
      ],
      "metadata": {
        "id": "T2XfjMU-XYxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "eYpXgFrr8wzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh_VoVaONh2Q",
        "outputId": "a0b05733-7f50-4752-ee73-053f5f64fa57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1981/1981 [==============================] - 234s 114ms/step - loss: 0.5287 - accuracy: 0.7029 - val_loss: 0.4815 - val_accuracy: 0.7365\n",
            "Epoch 2/20\n",
            " 901/1981 [============>.................] - ETA: 1:52 - loss: 0.4237 - accuracy: 0.7832"
          ]
        }
      ],
      "source": [
        "model.fit(x_train_dl, y_train, validation_data=(x_val_dl, y_val), epochs=20, batch_size=32, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tvPjSV7g7Dd"
      },
      "source": [
        "Let's evaluate our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW6FaaiypsOw"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test_dl, y_test)\n",
        "train_acc = model.evaluate(x_train_dl, y_train)[1]\n",
        "model.evaluate(x_val_dl, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration"
      ],
      "metadata": {
        "id": "vcJQAKHiLv4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Characters name by show\n",
        "characters_by_show_df = test_df.groupby('Show')['Character'].unique().reset_index()\n",
        "characters_by_show_df.columns = ['Show', 'Characters']\n",
        "characters_by_show_df"
      ],
      "metadata": {
        "id": "kS8-ZcsyMU4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "Rr-67ngERfjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities for each class\n",
        "predicted_probabilities = model.predict(x_test_dl)\n",
        "\n",
        "# Convert the predicted probabilities to predicted class labels\n",
        "predicted_classes = (predicted_probabilities > 0.5).astype(int)  # Assuming binary classification with threshold 0.5\n",
        "\n",
        "# Assuming that '1' corresponds to 'Friends' and '0' corresponds to 'Seinfeld'\n",
        "predicted_shows = ['Friends' if pred == 0 else 'Seinfeld' for pred in predicted_classes]"
      ],
      "metadata": {
        "id": "kF7EmcViJ4_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to count correct and incorrect classifications for each character\n",
        "def count_classification_errors(actual, predicted):\n",
        "    correct = sum(1 for a, p in zip(actual, predicted) if a == p)\n",
        "    incorrect = len(actual) - correct\n",
        "    return correct, incorrect"
      ],
      "metadata": {
        "id": "yBsYPmVuwrFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_show(characters, show_name, actuals, predictions):\n",
        "    correct_counts = []\n",
        "    incorrect_counts = []\n",
        "    correct_percentages = []\n",
        "    incorrect_percentages = []\n",
        "\n",
        "    for character in characters:\n",
        "        mask = (test_df['Character'] == character) & (test_df['Show'] == show_name)\n",
        "        character_actuals = actuals[mask]\n",
        "        character_predictions = [predictions[i] for i, m in enumerate(mask) if m]\n",
        "        correct, incorrect = count_classification_errors(character_actuals, character_predictions)\n",
        "\n",
        "        total = correct + incorrect\n",
        "        correct_counts.append(correct)\n",
        "        incorrect_counts.append(incorrect)\n",
        "        correct_percentages.append((correct / total) * 100)\n",
        "        incorrect_percentages.append((incorrect / total) * 100)\n",
        "\n",
        "    return correct_counts, incorrect_counts, correct_percentages, incorrect_percentages"
      ],
      "metadata": {
        "id": "LnrrCl_bwrLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_shows = test_df['Show']\n",
        "\n",
        "# Process Friends show\n",
        "friends_correct_counts, friends_incorrect_counts, friends_correct_percentages, friends_incorrect_percentages = process_show(characters_by_show_df['Characters'][0], 'Friends', actual_shows, predicted_shows)\n",
        "\n",
        "# Process Seinfeld show\n",
        "seinfeld_correct_counts, seinfeld_incorrect_counts, seinfeld_correct_percentages, seinfeld_incorrect_percentages = process_show(characters_by_show_df['Characters'][1], 'Seinfeld', actual_shows, predicted_shows)"
      ],
      "metadata": {
        "id": "eSZw3zZewrOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_show_statistics(show_name, correct_counts, incorrect_counts, correct_percentages, incorrect_percentages, characters):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bar_width = 0.4\n",
        "    indices = range(len(characters))\n",
        "    plt.bar(indices, correct_counts, width=bar_width, color='green', label=f'{show_name} - Correct')\n",
        "    plt.bar(indices, incorrect_counts, bottom=correct_counts, width=bar_width, color='red', label=f'{show_name} - Incorrect')\n",
        "    plt.xlabel('Character')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(f'Correct vs Incorrect Classifications for {show_name} Characters')\n",
        "    plt.xticks(indices, characters, ha='right')\n",
        "    plt.legend()\n",
        "\n",
        "    # Add percentage labels\n",
        "    for i, (correct, incorrect) in enumerate(zip(correct_percentages, incorrect_percentages)):\n",
        "        plt.text(i, correct_counts[i] + 5, f'{correct:.1f}%', ha='center', color='black')\n",
        "        plt.text(i, correct_counts[i] + incorrect_counts[i] + 5, f'{incorrect:.1f}%', ha='center', color='black')\n",
        "\n",
        "    # Print accuracies for each character\n",
        "    print(f'Accuracies for {show_name} Characters:')\n",
        "    for character, correct, incorrect in zip(characters, correct_counts, incorrect_counts):\n",
        "        total = correct + incorrect\n",
        "        accuracy = correct / total if total > 0 else 0.0\n",
        "        print(f'{character}: {accuracy:.2f}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9nf31g_swrQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot statistics for Friends\n",
        "plot_show_statistics('Friends', friends_correct_counts, friends_incorrect_counts, friends_correct_percentages, friends_incorrect_percentages, characters_by_show_df['Characters'][0])"
      ],
      "metadata": {
        "id": "DLNMNFqxwrS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot statistics for Seinfeld\n",
        "plot_show_statistics('Seinfeld', seinfeld_correct_counts, seinfeld_incorrect_counts, seinfeld_correct_percentages, seinfeld_incorrect_percentages, characters_by_show_df['Characters'][1])"
      ],
      "metadata": {
        "id": "0kjqBFrxwrVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report"
      ],
      "metadata": {
        "id": "NHyv5dQIw0tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test_dl, y_test)"
      ],
      "metadata": {
        "id": "6ay5-xQvwrX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "n6CH1Tr_wrab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {'Test set accuracy': [accuracy],\n",
        "           'Train set accuracy': [train_acc],\n",
        "           'Number of trainable parameters': '885893',\n",
        "           'Number of layers': '5',\n",
        "           'Regularization methods': 'Dropout',\n",
        "           'Number of epochs': '4',\n",
        "           'Choice of loss function': 'binary_crossentropy',\n",
        "           'Choice of optimizer': 'adam',\n",
        "           'Embedding dimension': '32'\n",
        "           }"
      ],
      "metadata": {
        "id": "CpIRSE-mwrcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results = pd.DataFrame(results)\n",
        "df_results"
      ],
      "metadata": {
        "id": "vPtlr26Iw59k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_results to CSV\n",
        "import os\n",
        "df_results.to_csv(os.path.join(os.getcwd(), 'exercise3.csv'))"
      ],
      "metadata": {
        "id": "-BV0t5dSw7It"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}